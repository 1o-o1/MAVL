# MAVL Configuration File

# Dataset Configuration
data:
  chexpert_root: "/path/to/CheXpert-v1.0-small"
  mimic_root: "/path/to/mimic-cxr"
  padchest_root: "/path/to/PadChest"
  
  train_csv: "train.csv"
  valid_csv: "valid.csv"
  test_csv: "test.csv"
  
  # Data splits
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1
  
  # Unseen disease percentages for zero-shot evaluation
  unseen_percentages: [0.02, 0.05, 0.1, 0.15, 0.25]
  
  # Image preprocessing
  image_size: 224
  normalize_mean: [0.485, 0.456, 0.406]  # ImageNet statistics
  normalize_std: [0.229, 0.224, 0.225]
  
  # Disease labels
  disease_labels: [
    "No Finding", "Enlarged Cardiomediastinum", "Cardiomegaly", "Lung Opacity",
    "Lung Lesion", "Edema", "Consolidation", "Pneumonia", "Atelectasis",
    "Pneumothorax", "Pleural Effusion", "Pleural Other", "Fracture", "Support Devices"
  ]

# Model Configuration  
model:
  # Vision Encoder
  vision_encoder_name: "vit_base_patch16_224"  # From timm
  vision_hidden_dim: 768
  vision_output_dim: 1024
  vision_dropout: 0.1
  
  # Text Encoder
  text_encoder_name: "emilyalsentzer/Bio_ClinicalBERT"
  text_hidden_dim: 768
  text_output_dim: 1024
  text_max_length: 512
  text_dropout: 0.1
  
  # Dual Head Configuration
  num_heads: 8
  num_aspect_queries: 10  # K aspect queries per disease
  fusion_hidden_dim: 512
  fusion_dropout: 0.5
  
  # Neural Memory
  memory_size: 100  # K memory slots
  memory_dim: 1024  # D dimension
  memory_lr: 0.01  # η
  memory_beta: 0.9  # β momentum
  memory_alpha: 0.1  # α update strength
  
  # Loss weights
  lambda_contrastive: 0.1  # λ for contrastive loss
  temperature: 0.1  # τ for InfoNCE loss

# Training Configuration
training:
  batch_size: 16
  num_epochs: 40
  learning_rate: 1e-4
  weight_decay: 1e-5
  grad_clip_norm: 5.0
  
  # Optimizer
  optimizer: "adam"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_eps: 1e-8
  
  # Learning rate scheduler
  scheduler: "cosine"
  warmup_epochs: 2
  min_lr: 1e-6
  
  # Training settings
  num_workers: 4
  pin_memory: true
  mixed_precision: true
  gradient_accumulation_steps: 1
  
  # Checkpointing
  save_frequency: 5  # Save every N epochs
  early_stopping_patience: 10
  early_stopping_metric: "val_auc"
  
# Report Generation Configuration
report_generation:
  model_name: "deepseek-r1:14b"  # Can be 7b, 14b, or 32b
  temperature: 0.7
  max_tokens: 256
  num_predict: 256
  
# Logging Configuration
logging:
  use_tensorboard: true
  use_wandb: false
  wandb_project: "mavl-lung-disease"
  wandb_entity: null
  log_frequency: 100  # Log every N steps
  
# Experiment Configuration
experiment:
  seed: 42
  deterministic: true
  output_dir: "./results"
  experiment_name: "mavl_baseline"